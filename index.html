<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="VARGPT: Unified Understanding and Generation in a Visual
    Autoregressive
    Multimodal Large Language Model.">
  <meta name="keywords" content="VARGPT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VARGPT</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>




  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VARGPT: Unified Understanding and Generation in a Visual
              Autoregressive
              Multimodal Large Language Model</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://keunhong.com">Xianwei Zhuang</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://utkarshsinha.com">Yuxin Xie</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://utkarshsinha.com">Yufan Deng</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://utkarshsinha.com">Liming Liang</a>,</span>
              <span class="author-block">
                <a href="https://utkarshsinha.com">Jinghan Ru</a>,</span>
              <span class="author-block">
                <a href="https://utkarshsinha.com">Yuguo Yin</a>,</span>
              <span class="author-block">
                <a href="https://jonbarron.info">Yuexian Zou</a><sup>†</sup>,
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Peking University</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block">xwzhuang@stu.pku.edu.cn</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/google/nerfies" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/google/nerfies/releases/tag/0.1"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">


      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abilities</h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Comparative analysis</h3>
          <div class="content has-text-justified">
            <p>
              A comparative analysis of various MLLMs across multi-
              ple visual comprehension and generation benchmarks is presented.
              The CLIP-scores is employed as a text-to-image visual genera-
              tion metric, while the remaining metrics are derived from stan-
              dard visual question-answering benchmarks and multi-modal com-
              prehension benchmarks. Notably, our VARGPT model demon-
              strates significant superiority over the compared baselines across
              all comprehension benchmarks. Furthermore, it exhibits excep-
              tional instruction-to-image generation capabilities, thus enhancing
              its versatility and applicability in diverse visual-linguistic tasks.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/vargptintro-methods.jpg" class="interpolation-image"
              alt="Interpolate start reference image." />
          </div>

          <h3 class="title is-4">Generated samples</h3>
          <div class="content has-text-justified">
            <p>
              Some generated 256×256 samples by VARGPT trained on ImageNet-1K. VARGPT supports text-and-image
              instructions from
              user and outputs both text-and-image mixed modal data simultaneously.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/VARGPT-vis.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>
        </div>
        <br />
        <!--/ Interpolating. -->

      </div>
    </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present VARGPT, a novel multimodal large language model (MLLM) that unifies visual understanding and
              generation within a single autoregressive framework. VARGPT employs a next-token prediction paradigm for
              visual understanding and a next-scale prediction paradigm for visual autoregressive generation. Building
              upon the LLaVA architecture, VARGPT incorporates a visual decoder and a multi-scale image tokenizer, along
              with two feature projectors for efficiently extending MLLMs to visual autoregressive generation and
              inherently support mixed-modal input and output.
              Our VARGPT undergoes a three-stage unified training process on three specially curated datasets,
              encompassing a pre-training phase and two mixed visual instruction fine-tuning phases.
              The unified training strategy are designed to achieve alignment between visual and textual features,
              enhance instruction following for both understanding and generation, and improve visual generation
              quality, respectively.
              Despite its LLAVA-based architecture for multimodel understanding, VARGPT significantly outperforms
              LLaVA-1.5 on various vision-centric benchmarks, such as visual question-answering and reasoning tasks.
              Notably, VARGPT also demonstrates remarkable capabilities in visual autoregressive generation and
              instruction-to-image synthesis, showcasing its versatility in both visual understanding and generation
              tasks.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">


      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Methodology</h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Model Architecture</h3>
          <div class="content has-text-justified">
            <p>
              The illustration of the proposed VARGPT framework, which consists of (1) a large language model, visual
              encoder and a
              understanding projector for visual understanding; (2) a visual decoder and dual generation projectors for
              visual generation. VARGPT
              employs the employs causal attention in the Large Language Model (LLM) backbone, while utilizing chunked
              causal attention in the visual
              decoder.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/VARGPT-main.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>

          <h3 class="title is-4">Training</h3>
          <div class="content has-text-justified">
            <p>
              The three training pipeline of the VARGPT, including stage-1 pretraining, stage-2 and stage-3 instruction
              fine-tuning.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/VARGPT-training.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>
        </div>
        <br />
        <!--/ Interpolating. -->

      </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">


      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Data</h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Data proportion in the three training stages.</h3>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/PieChart2.jpg" class="interpolation-image"
              alt="Interpolate start reference image." />
          </div>

          <h3 class="title is-4">Data distribution of the mixed instruction fine-tuning dataset.</h3>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/PieChart.jpg" class="interpolation-image"
              alt="Interpolate start reference image." />
          </div>
        </div>
        <br />
        <!--/ Interpolating. -->

      </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">


      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Zero-shot multi-modal evaluation</h3>
          <div class="content has-text-justified">
            <p>
              Zero-shot multi-modal evaluation on multi-modal benchmarks including MMMU, MME, MMBench, SEEDBench, and
              POPE (including different settings random, popular and adversarial ). The overall scores are reported for
              evaluation and we report test results for MMBench. Gen represents whether the method supports image
              generation capability. VARGPT achieves the best overall performance.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/experiment1.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>

          <h3 class="title is-4">Performance comparison on visual question answering tasks.</h3>
          <div class="content has-text-justified">
            <p>
              Performance comparison on visual question answering tasks. We gray out the model has trained on the
              dataset. Gen represents
              whether the method supports image generation capability.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/experiment2.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>
        </div>
        <br />
        <!--/ Interpolating. -->

      </div>
    </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">


      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">More samples</h2>
          <h3 class="title is-4">Visual understanding samples</h3>
          <div class="content has-text-justified">
            <p>
              The cases of visual understanding in VARGPT show that our VARGPT has achieved superior performance in
              understanding.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/visual_understanding_case1.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/visual_understanding_case2.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/visual_understanding_case3.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>
          <h3 class="title is-4">Generated samples</h3>
          <div class="content has-text-justified">
            <p>
              Some generated 256×256 samples by VARGPT trained on ImageNet-1K. VARGPT supports user text command input
              and
              outputs both text and image modal data simultaneously.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/VARGPT-exp-vis.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/VARGPT-appendix-gen-1_page_1.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/vargpt_images/VARGPT-appendix-gen-2_page_1.jpg" class="interpolation-image-2"
              alt="Interpolate start reference image." />
          </div>

        </div>
        <br />
        <!--/ Interpolating. -->

      </div>


    </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
    </div>
  </section>

  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under
        a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>. We thank the LLaMA team for giving us access to
        their models, and open-source projects, including Alpaca and Vicuna.
      </p>

      <p>
        <b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only.
        They are also restricted to uses that follow the license agreement of CLIP, LLaMA, Vicuna and GPT-4. The dataset
        is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used
        outside of research purposes.
      </p>

      <p>
        <a href="https://github.com/Computer-Vision-in-the-Wild/"><img id="painting_icon" width="10%"
            src="https://avatars.githubusercontent.com/u/97258247?s=200&amp;v=4"></a>
        Related Links:
        <a href="https://react-vl.github.io/">[REACT]</a>
        <a href="https://gligen.github.io/">[GLIGEN]</a>
        <a href="https://github.com/Computer-Vision-in-the-Wild/">[Computer Vision in the Wild (CVinW)]</a>
        <a href="https://instruction-tuning-with-gpt-4.github.io/">[Insutrction Tuning with GPT-4]</a>
      </p>
    </div>
  </section>

  <footer class="footer">
  </footer>

</body>

</html>
